# -*- coding: utf-8 -*-
"""Finetune-distilbert

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JzQ0e607tK6-kTmc1xKuG6Ggo8h9-me5
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams['figure.figsize'] = (12,6) # set the plot size

data=pd.read_csv('nike_data_2022_09.csv')
data.head()

from google.colab import drive
drive.mount('/content/drive')

data.shape

data.info()

num_duplicates_data = data.duplicated().sum()
prop_duplicates_data = data.duplicated().mean()

print("Number of duplicate observations in the dataset:", num_duplicates_data)
print("Proportion of duplicate observations in the dataset:", prop_duplicates_data)

data = data.drop_duplicates()

data = data.reset_index(drop=True)

from sklearn.model_selection import train_test_split

train_d, test_d = train_test_split(data, random_state=3)
tr_idx = train_d.index
tst_idx = test_d.index
train_d.shape, test_d.shape

data.iloc[tr_idx].describe()

print("Number of rows in the training data:", len(tr_idx))
print("Number of rows in the test data:", len(tst_idx))

def classify(x):
    if x < 3.5:
        return "neg"
    else:
        return "pos"
    
    
data['overall_rating'] = data['avg_rating'].apply(classify)

data.iloc[tr_idx]['overall_rating'].value_counts()

fig, ax = plt.subplots(1,2)
sns.histplot(x=data.iloc[tr_idx]['price'], hue=data.iloc[tr_idx]['overall_rating'], kde=True, ax=ax[0])
sns.histplot(x=data.iloc[tr_idx]['avg_rating'], hue=data.iloc[tr_idx]['overall_rating'], kde=True, ax=ax[1])

ax[0].title.set_text('Histogram of price by sentiment category')
ax[1].title.set_text('Histogram of rating by sentiment category')

sns.boxplot(x=data.iloc[tr_idx]['price'], y=data.iloc[tr_idx]['overall_rating'])

data.iloc[tr_idx].sort_values(by='price').nlargest(3, 'price')

numeric_data = data.select_dtypes(exclude='object')
correlation = numeric_data.corr()
sns.heatmap(correlation, annot=True, cbar=False)

#PRODUCT DESCRIPTION
# number of words per product description
def words_count(x):
    return len(x.split())

data['prod_desc_num_words']= data['description'].apply(words_count)

fig, ax = plt.subplots(1,2)
sns.histplot(x=data.iloc[tr_idx]['prod_desc_num_words'], hue=data.iloc[tr_idx]['overall_rating'], 
             kde=True, stat='density', linewidth=0, ax=ax[0])
sns.boxplot(y=data.iloc[tr_idx]['prod_desc_num_words'], x=data.iloc[tr_idx]['overall_rating'], ax=ax[1])

ax[0].set_title('Histogram of Number of Words per product description')
ax[0].set_xlabel('Number of words')
ax[1].set_title('Boxplots of Words per product description')
ax[1].set_xlabel('Product overall category')
ax[1].set_ylabel('Number of words')

def avg_leng_word(x):
    return np.sum([len(w) for w in x.split()]) / len(x.split())

data['avg_prod_desc_length'] = data['description'].apply(avg_leng_word)
sns.histplot(data.iloc[tr_idx]['avg_prod_desc_length'], kde=True, stat='density', linewidth=0)
plt.title('Histogram of Average Length of Word in product description')
plt.xlabel('Average length of word')


fig, ax = plt.subplots(1,2)
sns.histplot(x=data.iloc[tr_idx]['avg_prod_desc_length'], hue=data.iloc[tr_idx]['overall_rating'], 
             kde=True, stat='density', linewidth=0, ax=ax[0])
sns.boxplot(y=data.iloc[tr_idx]['avg_prod_desc_length'], x=data.iloc[tr_idx]['overall_rating'], ax=ax[1])

ax[0].set_title('Histogram of Average Length of Word in product description')
ax[0].set_xlabel('Average length of word')
ax[1].set_title('Boxplots of Average Length of Word in product description')
ax[1].set_xlabel('Product overall category')
ax[1].set_ylabel('Average length of word')

from collections import defaultdict, Counter
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords', quiet=True)
stopwords = stopwords.words('english')


# Create a corpus of words in product description
def create_corpus(column_name):
    corpus = []
    for w in data['{0}'.format(column_name)].str.split():
        for i in w:
            corpus.append(i.lower())
    return corpus


# Create a corpus of stopwords in product description
def create_corpus_dict(column_name):
    corpuse = create_corpus(column_name)
    stop_dict = defaultdict(int)
    for word in corpuse:
        if word in stopwords:
          stop_dict[word] += 1
    return sorted(stop_dict.items(), key=lambda x: x[1],
                  reverse=True)

corpuse_prod_desc_non_stop = create_corpus('description')
corpuse_prod_desc_stop = create_corpus_dict('description')

stop_prod_desc_x, stop_prod_desc_y = zip(*corpuse_prod_desc_stop)

# non-stopwords
counter_prod_desc = Counter(corpuse_prod_desc_non_stop)

prod_desc_x = []
prod_desc_y = []
counter = 0
for word, count in counter_prod_desc.most_common()[:100]:
    if (counter < 30) and (word not in stopwords):
        counter += 1
        prod_desc_x.append(word)
        prod_desc_y.append(count)

fig, ax = plt.subplots(1, 2, figsize=(16, 8))
sns.barplot(y=list(stop_prod_desc_x)[0:30], x=list(stop_prod_desc_y)[0:30],
            orient='h', palette='Reds', ax=ax[0])
sns.barplot(x=prod_desc_y, y=prod_desc_x, orient='h',
            palette='Blues', ax=ax[1])

ax[0].title.set_text('30 most common stopwords in product description')
ax[1].title.set_text('30 most common unigram NON-stopwords in product description')

# delete everything except alphabet
data['description'] = data['description'].str.replace("[^a-zA-Z#]", " ")

# convert to lowercase
data['description'] = data['description'].str.lower()

import nltk
import string

nltk.download('words')
words = set(nltk.corpus.words.words()) # english words
string.punctuation


# remove non-english words.
def remove_non_english(row):
    result = []
    for text in row:
        new_text = " ".join(w for w in text.split() if w in words)
        result.append(new_text)
    return result

# remove non-printable characters.
def remove_not_ASCII(row):
    result = []
    for text in row.split('||'):
        new_text = ''
        for word in text.split():
            word = ''.join([char for char in word if char in string.printable])
            new_text += word + ' '
        result.append(new_text[:-1])
    return result


# get rid of the punctuation.
def remove_all_punct(row):
    result = []
    for text in row:
        table = str.maketrans('', '', string.punctuation)
        new_text = text.translate(table)
        result.append(new_text)
    return result

# lower text.
def lower_text(row):
    result = []
    for text in row:
        new_text = text.lower()
        result.append(new_text)
    return result

# remove numbers.
def remove_numbers(row):
    result = []
    for text in row:
        num = re.compile(r'[-+]?[.\d]*[\d]+[:,.\d]*')
        new_text = num.sub(r"NUMBER", text)
        result.append(new_text)
    return result

# remove emojis.
def remove_emojis(row):
    result = ''
    for text in row.split('||'):
        emoji = re.compile("["u"\U0001F600-\U0001F64F""]+") # emojis
        new_text = emoji.sub('rEMOJI', text)
        result += new_text + '||'
    return result


# remove english stopwords.
def remove_stopwords(row): # should we remove 'not'?
    result = []
    for text in row:
        non_stop = ' '.join(word for word in text.split() if word not in stops)
        result.append(non_stop)
    return result

def clean_text(row):
    #row = split_text(row)
    row = remove_not_ASCII(row)
    row = remove_all_punct(row)
    row = remove_numbers(row)
    row = lower_text(row)
    row = remove_stopwords(row)
    row = remove_non_english(row)
    #row = remove_emojis(row)
    return row

#MODEL TRAINING
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

# get the statistics from feeding our models.
def fit(model, x_train, x_test, y_train, y_test, vectoriser):
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    cmatrix = confusion_matrix(y_test, y_pred)
    sns.heatmap(cmatrix, cbar=False)
    plt.xlabel("y_predict")
    plt.ylabel("y_true")
    
    f1score = f1_score(y_test, y_pred, average='weighted')
    train_acc = round(model.score(x_train, y_train)*100)
    test_acc = round(accuracy_score(y_test,y_pred)*100)
    
    print(str(model.__class__.__name__) +" using "+ str(vectoriser))
    print('Accuracy of classifier on training set:{}%'.format(train_acc))
    print('Accuracy of classifier on test set:{}%' .format(test_acc))

sns.boxplot(x=data.iloc[tr_idx]['price'], y=data.iloc[tr_idx]['overall_rating'])

def words_count(x):
    return len(x.split())

data['prod_desc_num_words']=data['description'].apply(words_count)

fig, ax = plt.subplots(1,2)
sns.histplot(x=data.iloc[tr_idx]['prod_desc_num_words'], hue=data.iloc[tr_idx]['overall_rating'], 
             kde=True, stat='density', linewidth=0, ax=ax[0])
sns.boxplot(y=data.iloc[tr_idx]['prod_desc_num_words'], x=data.iloc[tr_idx]['overall_rating'], ax=ax[1])

ax[0].set_title('Histogram of Number of Words per product description')
ax[0].set_xlabel('Number of words')
ax[1].set_title('Boxplots of Words per product description')
ax[1].set_xlabel('Product overall category')
ax[1].set_ylabel('Number of words')

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
X, y = np.arange(10).reshape((5, 2)), range(5)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Standardize the data set
#
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
#
# Fit the SVC model
#
svc = SVC(kernel='linear', C=10.0, random_state=1)
svc.fit(X_train, y_train)
#
# Get the predictions
#
y_pred = svc.predict(X_test)
#
# Calculate the confusion matrix
#
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()